---
title: P-values and control of FWER
author: ''
date: '2019-02-24'
categories:
  - Basics
slug: P-values and control of FWER
---



<div id="derivation" class="section level3">
<h3>Derivation</h3>
<p>P-values are ubiquitous in the world of inference. Formally, a <em>p-value</em> is constructed as follows. Suppose we observe (for simplicity, real-valued) data <span class="math display">\[
Y = (Y_1, Y_2, \ldots, Y_n) \; \; \sim \; \; P_\theta
\]</span> where, as experienced modelers, we have settled on some family of candidate probability measures on <span class="math inline">\(\mathbb{R}^n\)</span> parameterized by the vector <span class="math inline">\(\theta\)</span>. We may wish to test the credibility of a null hypothesis <span class="math display">\[
\mathcal{H}_0 : \;\; \theta \in \Theta_0
\]</span> and suppose that we have cleverly identified a real-valued statistic <span class="math inline">\(g(Y)\)</span> whose distribution <em>under this null hypothesis</em> is fully-specified; i.e. we can unambiguously compute <span class="math display">\[
P_0(B) \doteq \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}}\Big[g(Y) \in B \Big]
\]</span> and we write <span class="math inline">\(g(Y) \sim P_0\)</span>, where <span class="math inline">\(P_0\)</span> doesn’t depend on the unknown <span class="math inline">\(\theta\)</span> anymore. For the remainder of this post, we assume that <span class="math inline">\(g(Y)\)</span> is a continuous random variable; i.e. <span class="math inline">\(P_0\)</span> is absolutely continuous with respect to the Lebesgue measure on <span class="math inline">\(\mathbb{R}\)</span>.</p>
<p>When should we reject the null hypothesis? It would be sensible to do so if we observe a “rare” value of <span class="math inline">\(g(Y)\)</span> under the null distribution <span class="math inline">\(P_0\)</span>. Usually we take the significance level <span class="math inline">\(\alpha = .05\)</span> as our “rare” threshold. So we might define a rejection region of the form <span class="math inline">\((t, \infty)\)</span> such that <span class="math display">\[
P_0\Big((t, \infty)\Big) = \text{Pr}_{_{Z\sim P_0}} \Big[Z &gt; t \Big] = \alpha\,\,.
\]</span> From this, we see that the statistic that we computed on the data satisfies <span class="math display">\[
g(Y) &gt; t  \;\; \iff \;\; \text{Pr}_{_{Z\sim P_0}}\Big[Z &gt; g(Y) \Big] \le \alpha\,\,.
\]</span> This, then, yields the <u>definition of p-value</u>: <span class="math display">\[
p(Y) \doteq \text{Pr}_{_{Z\sim P_0}}\Big[Z &gt; g(Y) \Big] = 1 - F_0(g(Y))\,\,.
\]</span> where <span class="math inline">\(F_0\)</span> is the (continuous) distribution function for the measure <span class="math inline">\(P_0\)</span>. It is fruitful to compare this with the colloquial interpretation: <strong>“the p-value is the chance of observing, on many similar datasets, an effect at least as extreme as the one in our sample data, assuming the null hypothesis is true”</strong>.</p>
<p>We can see that under the null hypothesis, the p-value follows a uniform distribution on <span class="math inline">\([0,1]\)</span> since for any <span class="math inline">\(t\in [0,1]\)</span> we have <span class="math display">\[
\begin{align*}
\text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[ p(Y) \le t\Big] &amp;= 
\text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[ 1 - F_0(g(Y)) \le t\Big] \\[5px]
&amp;= \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[F_0(g(Y)) \ge 1-t\Big] \\[5px]
&amp;= 1 - \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[F_0(g(Y)) &lt; 1-t\Big] \\[5px]
&amp;= 1 - \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[g(Y) \le F_0^{-1}(1-t)\Big] \\[5px]
&amp;= 1 - F_0(F_0^{-1}(1-t)) \\[5px]
&amp;= t
\end{align*}
\]</span> where the last step follows by continuity of <span class="math inline">\(F_0\)</span>. And clearly this is precisely the distribution function of the <span class="math inline">\(\text{unif}(0,1)\)</span> measure.</p>
<p>So if we reject the null hypothesis when <span class="math inline">\(p(Y) \le \alpha\)</span>, we do so with probability <span class="math inline">\(\alpha\)</span> under the null model. Therefore, in this setting the Type 1 error, or <strong>“the probability of rejecting the null when the null is true”</strong>, is <span class="math inline">\(\alpha\)</span>.</p>
</div>
<div id="bonferroni-correction" class="section level3">
<h3>Bonferroni correction</h3>
<p>Now let’s take a look at what happens when we conduct multiple tests simultaneously. In other words, from our sample data, we would like to determine whether there is evidence enough to reject a collection of null hypotheses. For each null hypothesis <span class="math inline">\(\mathcal{H}_i\)</span> we formulate a statistic <span class="math inline">\(g_i(Y)\)</span> and compute the corresponding p-value <span class="math inline">\(p_i(Y)\)</span>: <span class="math display">\[
\begin{alignat*}{4}
&amp;\mathcal{H}_1\qquad &amp;&amp;\mathcal{H}_2 \qquad &amp;&amp;\ldots \qquad &amp;&amp;\mathcal{H}_m \\[5px]
&amp;\downarrow &amp;&amp; \downarrow  &amp;&amp; &amp;&amp; \downarrow \\[5px]
&amp; p_1(Y) &amp;&amp; p_2(Y) &amp;&amp; &amp;&amp; p_m(Y)
\end{alignat*}
\]</span></p>
<p>For simplicity, suppose that we insist on performing a test of the same form as the single hypothesis setting discussed above: we reject those null hypotheses for which <span class="math inline">\(p_i(Y) \le \gamma\)</span> for some predetermined <span class="math inline">\(\gamma\)</span>. What should we pick as our <span class="math inline">\(\gamma\)</span>? This depends on the kind of control we wish to achieve. Previously, with a single hypothesis, we were controlling the <em>probability of rejecting a true null</em>. A natural generalization of this principle would be to control the probability of rejecting any true nulls; in other words, we could control the <em>probability of rejecting at least one true null</em>. This is known as the <strong>familywise error rate (FWER)</strong>.</p>
<p>Let’s formalize this concept. Null hypotheses are <u>statements about the location of the true underlying parameter <span class="math inline">\(\theta\)</span> </u>. For example, if the parameter space is <span class="math inline">\(\mathbb{R}^2\)</span>, null hypotheses correspond to subsets of the plane with each hypothesis asserting that <span class="math inline">\(\theta\)</span> lies in its subset. In the picture below, the black dot is <span class="math inline">\(\theta\)</span> so we see that <span class="math inline">\(2\)</span> of the <span class="math inline">\(3\)</span> null hypotheses are true.</p>
<iframe src="https://www.desmos.com/calculator/4laiy3c8is?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder="0">
</iframe>
<p>If <span class="math inline">\(m_0\)</span> out of the <span class="math inline">\(m\)</span> null hypotheses are true, we can write the FWER as follows:</p>
<p><span class="math display">\[
\begin{align*}
\text{FWER} &amp;= \text{Pr}_{_{Y\sim P_\theta}} \Big[ \text{reject at least one true null} \Big] \\[5px] 
&amp;= \text{Pr}_{_{Y\sim P_\theta}} \Bigg[\bigcup_{i: \,\mathcal{H_i} \text{ true}} \Big\{p_i(Y) \le \gamma \Big\} \Bigg] \\[5px]
&amp;\le \sum_{i: \,\mathcal{H_i} \text{ true}} \text{Pr}_{_{Y\sim P_\theta, \,\mathcal{H}_i}} \Big[ p_i(Y) \le \gamma  \Big] \\[5px]
&amp;= m_0\gamma\,\,.
\end{align*}
\]</span> So to control the FWER at level <span class="math inline">\(\alpha\)</span>, it suffices to select <span class="math inline">\(\gamma\)</span> such that <span class="math inline">\(m_0 \gamma \le \alpha\)</span>. However, we don’t know <span class="math inline">\(m_0\)</span>! Despite this, since <span class="math inline">\(m_0 \le m\)</span>, we can make the conservative choice <span class="math inline">\(\gamma = \frac{\alpha}{m}\)</span> which satisfies the desired inequality: <span class="math display">\[
\text{FWER} \le m_0 \gamma = m_0 \bigg(\frac{\alpha}{m}\bigg) \le m \bigg( \frac{\alpha}{m} \bigg) = \alpha \,\,.
\]</span> This procedure usually controls the FWER at a level substantially lower than <span class="math inline">\(\alpha\)</span>, so we won’t be rejecting many nulls.</p>
</div>
<div id="holm-method" class="section level3">
<h3>Holm method</h3>
<p>This procedure is uniformly more powerful than the Bonferroni correction, in the sense that it still controls the FWER at the <span class="math inline">\(\alpha\)</span> level, but if Bonferroni rejects a particular null then so does Holm. We will elaborate on this point further, but first we present the procedure itself. For ease of notation, we will suppress the p-value’s dependence on the data and write <span class="math inline">\(p_i\)</span> instead of <span class="math inline">\(p_i(Y)\)</span>.</p>
<hr />
<ol style="list-style-type: decimal">
<li><p>Sort the <span class="math inline">\(m\)</span> p-values in ascending order: <span class="math inline">\(p_{(1)} \le p_{(2)} \le \cdots \le p_{(m)}\)</span>, with corresponding null hypotheses <span class="math inline">\(\mathcal{H}_{(1)}, \mathcal{H}_{(2)}, \cdots, \mathcal{H}_{(m)}\)</span>.</p></li>
<li>For a fixed significance level <span class="math inline">\(\alpha\)</span>, let <span class="math inline">\(k\)</span> be the minimal index such that <span class="math display">\[p_{(k)} &gt; \frac{\alpha}{m - (k-1)}\,\,.\]</span> Let’s set <span class="math inline">\(k = 0\)</span> if no such index exists. Note that by definition <span class="math inline">\(k\)</span> is <em>random</em> and depends on the data <span class="math inline">\(Y\)</span>.</li>
<li><p>Reject <span class="math inline">\(\mathcal{H}_{(1)}, \ldots, \mathcal{H}_{(k-1)}\)</span> and do not reject <span class="math inline">\(\mathcal{H}_{(k)}, \ldots , \mathcal{H}_{(m)}\)</span>. Note that the corresponding p-values satisfy <span class="math display">\[
p_{(1)} \le \frac{\alpha}{m}, \quad p_{(2)} \le \frac{\alpha}{m-1}, \quad \ldots \quad,  \quad p_{(k-1)} \le  \frac{\alpha}{m -(k-2)}, \quad p_{(k)} &gt;  \frac{\alpha}{m - (k-1)}\,.
\]</span> If <span class="math inline">\(k=1\)</span> then we don’t reject any hypotheses, and if <span class="math inline">\(k=0\)</span>, then we reject all of the hypotheses.</p></li>
</ol>
<hr />
<p>The steps are simple enough, but does this achieve our goal of controlling the FWER? To see this, let <span class="math inline">\(t\)</span> be the index of the <u>first rejected true hypothesis</u>, with respect to the sorted order of p-values above. To be thorough, we set <span class="math inline">\(t = 0\)</span> if no true hypotheses are rejected. This is another <em>random index</em>, and if <span class="math inline">\(t \ne 0\)</span>, it will satisfy <span class="math display">\[
p_{(t)} \le \frac{\alpha}{m - (t - 1)} \le \frac{\alpha}{m - (m - m_0)} = \frac{\alpha}{m_0} 
\]</span> since the first <span class="math inline">\(t-1\)</span> rejected hypotheses are false null hypotheses and there are <span class="math inline">\(m - m_0\)</span> of these in total.</p>
<p>Therefore, we have the following implication: if any true null hypotheses are rejected, the smallest of the corresponding p-values must be <span class="math inline">\(\le \frac{\alpha}{m_0}\)</span>. Even more generally, at least one of the <span class="math inline">\(m\)</span> p-values must be <span class="math inline">\(\le \frac{\alpha}{m_0}\)</span>. This yields the desired result: <span class="math display">\[
\begin{align*}
\text{FWER} &amp;= \text{Pr}_{_{Y\sim P_\theta}} \Big[ \text{reject at least one true null} \Big] \\[5px]
&amp;= \text{Pr}_{_{Y\sim P_\theta}} \Big[ t \ne 0 \Big] \\[5px]
&amp;\le \text{Pr}_{_{Y\sim P_\theta}} \Bigg[\bigcup_{i: \,\mathcal{H_i} \text{ true}} \bigg\{p_i \le \frac{\alpha}{m_0} \bigg\} \Bigg] \\[5px]
&amp;\le \sum_{i: \,\mathcal{H_i} \text{ true}} \text{Pr}_{_{Y\sim P_\theta, \,\mathcal{H}_i}} \bigg[ p_i \le \frac{\alpha}{m_0}  \bigg] \\[5px]
&amp;= m_0 \bigg(\frac{\alpha}{m_0} \bigg) \\[5px]
&amp;= \alpha \;\;.
\end{align*}
\]</span></p>
<p>Let’s return briefly to our claim that Holm is more powerful than Bonferroni; we can compare the regions corresponding to “rejection of at least one true null” between the two procedures and see that</p>
<p><span class="math display">\[
\left(\bigcup_{i: \,\mathcal{H_i} \text{ true}} \bigg\{p_i\le \frac{\alpha}{m} \bigg\} \right)\subseteq
\Big\{ t \ne 0  \Big\}\;\;.
\]</span> Or in words: if <span class="math inline">\(p_i \le \frac{\alpha}{m}\)</span> for some true hypothesis <span class="math inline">\(\mathcal{H}_i\)</span> causing a Bonferroni rejection, then <span class="math inline">\(t\ge 1\)</span> and in particular, <span class="math inline">\(\mathcal{H}_i\)</span> is rejected by Holm by inspection of step (3.) of the procedure.</p>
</div>
