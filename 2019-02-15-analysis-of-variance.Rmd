---
title: Analysis of variance
author: ''
date: '2019-02-15'
slug: analysis-of-variance
categories: ['Basics']
tags: []
subtitle: ''
---

## F-test

The F distribution will play an important role in the general framework of "analysis of variance" (ANOVA). Let's start with a simple scenario in which the F-test might arise. Suppose we observe (real-valued) data samples from several different groups

$$
\begin{align*}
\text{group }1 &: Y_1^{(1)}, Y_2^{(1)}, \ldots, Y_{n_1}^{(1)} \\
\text{group }2 &: Y_1^{(2)}, Y_2^{(2)}, \ldots, Y_{n_2}^{(2)} \\
&\vdots \\
\text{group }k &: Y_1^{(K)}, Y_2^{(K)}, \ldots, Y_{n_k}^{(k)} \\
\end{align*}
$$

and furthermore, we assume that the samples are independent draws from $k$ different Gaussians all with the same variance but potentially different means. The standard notation for this model is

$$ Y^{(i)}_j = \mu_i + \epsilon^{(i)}_j\qquad i=1,\ldots,k \qquad j=1,\ldots, n_i \qquad \epsilon^{(i)}_j \sim \mathcal{N}(0, \sigma^2)\,\,.$$

Here is an example of some data generated from this model with $k=4$ different group means and $\sigma = 1$ (with some added jittering):

```{r, echo=FALSE}
set.seed(1234)

# generate data
k = 4
means = rnorm(k)

draws = data.frame(group=0, x=0, y=0)
n = 100
for(i in 1:n) {
  group = sample(1:k, 1)
  draws[i, 1] = group
  draws[i, 2:3] = c(rnorm(1, mean=means[group]), 0)
}

# imitate the default color sequence in ggplot2
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}

# plot samples with overlayed true densities
library(ggplot2)
draws$group = as.factor(draws$group)
cols = gg_color_hue(k)
ggplot(draws, aes(x=x, y=y, col=group)) + 
  geom_hline(yintercept = 0) + 
  geom_point(position=position_jitter(width=0, height=.01)) + 
  ylim(-.15, .15) + 
  geom_vline(xintercept = means[1], linetype="dashed", color = cols[1]) + 
  geom_vline(xintercept = means[2], linetype="dashed", color = cols[2]) + 
  geom_vline(xintercept = means[3], linetype="dashed", color = cols[3]) + 
  geom_vline(xintercept = means[4], linetype="dashed", color = cols[4]) + 
  theme(axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(), 
        axis.title.x=element_blank()) + 
  scale_x_continuous(breaks=scales::pretty_breaks(n=8)) +
  guides(col=FALSE)

```


A reasonable question would be to ask if these data provide enough evidence to conclude that the group means are different. In this setting, we can employ the following __one-way ANOVA__ F-test statistic:

$$
F = \frac{\text{between-group variability}}{\text{within-group variability}} = \frac{\frac{1}{k-1}\left[\sum_{i=1}^k n_i (\bar{Y}^{(i)} - \widetilde{Y} )^2\right]} {\frac{1}{n-k}\left[\sum_{i=1}^k\sum_{j=1}^{n_i}(Y^{(i)}_j - \bar{Y}^{(i)})^2\right]} \tag{*}
$$

where $n = n_1 + \ldots + n_k$, $\widetilde{Y}$ is the average of all samples, and $\bar{Y}^{(i)}$ is the average of samples within the $i^{th}$ group. 

The numerator is a weighted sum of squared differences between the group means and the overall mean; the denominator is a sum of squared differences between each sample and its respective group mean. Intuitively, if the between-group variability is _substantially higher_ than the within-group variability, the $K$ groups or clusters are "separate enough" and we would have reason to believe that the means of the groups are not all the same. 

Under the typical ANOVA null hypothesis, we assume that the means of the $k$ groups are equal
$$
\mu_1 = \mu_2 = \cdots = \mu_k\,\,.
$$
In this setting, the F-test statistic above follows an F-distribution on $d_1 = k-1$ and $d_2 = n-k$ degrees of freedom. We reject the null if the calculated statistic is greater than $F_{k-1, n-k, \alpha}$, the $1-\alpha$ quantile of the $F_{k-1, n-k}$ distribution. 


### Derivation of null distribution

We first recall two facts:

*** 

1. the F-distribution on $d_1$ and $d_2$ degrees of freedom arises from the ratio of two independent chi-squared random variables normalized by their respective degrees of freedom; i.e. if $U_1 \sim \chi^2_{d_1}$ and $U_2 \sim \chi^2_{d_2}$ are independent, then 
$$
\frac{U_1/d_1}{U_2/d_2} \sim F_{d_1, d_2}\,.
$$

2. the chi-squared distribution on $d$ degrees of freedom arises from the sum of $d$ independent, standard normal random variables; i.e. if $Z_i \sim \mathcal{N}(0, 1)$ for $i = 1,\ldots,d$ are independent, then 
$$
\sum_{i=1}^d Z_i^2 \sim \chi^2_d\,.
$$

***

The goal is to show that $(*)$ is in form described in (item 1.) above. We will need a little bit of linear algebra to achieve this. Let's define the following (random) vectors of length $n$:
$$
\begin{alignat*}{4}
\mathbf{Y} &= (Y^{(1)}_1, Y^{(1)}_2, \ldots, Y^{(1)}_{n_1},\quad && Y^{(2)}_1, Y^{(2)}_2, \ldots,Y^{(2)}_{n_2},\quad && \ldots, \quad && Y^{(k)}_1, Y^{(k)}_2, \ldots ,Y^{(k)}_{n_k}) \\[5pt]
\bar{\mathbf{Y}} &= ( \underbrace{\bar{Y}^{(1)} ,\quad \ldots,\quad \bar{Y}^{(1)}}_{n_1 \text{ times}}, \quad && \underbrace{\bar{Y}^{(2)},\quad \ldots,\quad \bar{Y}^{(2)}}_{n_2 \text{ times}}, \quad &&\ldots, \quad  &&\underbrace{\bar{Y}^{(k)},\quad \ldots,\quad \bar{Y}^{(k)}}_{n_k \text{ times}})    \\[5pt]
\widetilde{\mathbf{Y}} &= (\widetilde{Y}, \ldots \quad\ldots \quad \ldots && \ldots \quad \ldots \quad \ldots &&\ldots &&\ldots \quad \ldots \quad \ldots, \widetilde{Y})
\end{alignat*}
$$ 
where $\mathbf{Y}$ is a vector of the original data, $\bar{\mathbf{Y}}$ is a vector of within-group averages, and $\widetilde{\mathbf{Y}}$ is a vector of the overall average. It's clear that $\bar{\mathbf{Y}}$ and $\widetilde{\mathbf{Y}}$ are linear combinations of the original, but to make this more precise, consider the following (fixed) vectors in $\mathbb{R}^n$:
$$
\begin{align*}
\mathbf{v}_1 &= (\underbrace{1, \ldots,  1}_{n_1 \text{ ones}}, \quad \underbrace{0, \ldots, 0}_{\text{ rest are zeros}}) \\[5px]
\mathbf{v}_2 &= (\underbrace{0, \ldots, 0}_{n_1 \text{ zeros}}, \quad \underbrace{1, \ldots, 1}_{n_2 \text{ ones}}, \quad\underbrace{0, \ldots, 0}_{\text{rest are zeros}}) \\[5px]
\mathbf{v}_3 &= (\underbrace{0, \ldots, 0, \quad 0, \ldots, 0}_{n_1 + n_2 \text{ zeros}}, \quad\underbrace{1, \ldots, 1}_{n_3 \text{ ones}}, \quad \underbrace{0, \ldots, 0}_{\text{rest are zeros}}) \\
&\vdots \\[5px]
\mathbf{v}_k &= (\underbrace{0, \ldots, 0}_{n - n_k \text{ zeros}},\quad \underbrace{1, \ldots, 1}_{n_k \text{ ones}}) \\[7px]
\mathbf{u} &= \mathbf{v}_1 + \cdots + \mathbf{v}_k = (\underbrace{1, \ldots, 1}_{n \text{ ones}})\,\,.
\end{align*}
$$

Now we make two important observations: $\bar{\mathbf{Y}}$ is the orthogonal projection of the data $\mathbf{Y}$ onto $\textit{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_2, \ldots, \mathbf{v}_k\}$, and $\widetilde{\mathbf{Y}}$ is the orthogonal projection of the data $\mathbf{Y}$ onto $\textit{span}\{\mathbf{u}\}$. We can denote this succinctly as 
$$
\bar{\mathbf{Y}} = A_{\mathbf{v}} \mathbf{Y} \quad,\quad \widetilde{\mathbf{Y}} = A_{\mathbf{u}} \mathbf{Y}
$$
where $A_{\mathbf{v}}$ and $A_{\mathbf{u}}$ are the appropriate orthogonal projection matrices (so they will be symmetric and idempotent). The pre-scaled numerator of the F-test statistic $(*)$ can now be written  
$$
\sum_{i=1}^k n_i (\bar{Y}^{(i)} - \widetilde{Y} )^2 = \lVert \bar{\mathbf{Y}} - \widetilde{\mathbf{Y}}\rVert^2 = \lVert A_\mathbf{v}\mathbf{Y} - A_\mathbf{u}\mathbf{Y}\rVert^2
$$ 
and the pre-scaled denominator can be written
$$
\sum_{i=1}^k\sum_{j=1}^{n_i}(Y^{(i)}_j - \bar{Y}^{(i)})^2 = \lVert \mathbf{Y} - \bar{\mathbf{Y}} \rVert^2 = \lVert \mathbf{Y} - A_\mathbf{v}\mathbf{Y}\rVert^2\,\,.
$$
As a geometric side note, since $\textit{span}\{\mathbf{u}\} \subset \textit{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$, the orthogonal projections above yield a pythagorean identity 
$$
\lVert \mathbf{Y} - \widetilde{\mathbf{Y}}\rVert^2 = \lVert \mathbf{Y} - \bar{\mathbf{Y}}\rVert^2 + \lVert \bar{\mathbf{Y}} - \widetilde{\mathbf{Y}}\rVert^2\,\,.
$$
This can conveniently be interpreted as a partition of variability: we are splitting the variation in all the data into variation *within* groups and variation *between* groups. We are now tasked with showing that the random variables on right-hand side are (stochastically) independent and follow chi-squared distributions. The independence follows easily from assumptions of the data generation model and the null hypothesis (which purports that the group means are equal, i.e. the data are iid):
$$
\begin{align*}
\text{cov}(\mathbf{Y} - \bar{\mathbf{Y}}, \bar{\mathbf{Y}} - \widetilde{\mathbf{Y}}) &= 
\text{cov}((I - A_\mathbf{v})\mathbf{Y}, (A_\mathbf{v} - A_\mathbf{u})\mathbf{Y}) \\[5px]
&= (I - A_\mathbf{v})\text{cov}(\mathbf{Y}, \mathbf{Y})(A_\mathbf{v} - A_\mathbf{u})^\intercal \\[5px]
&= (I - A_\mathbf{v})\sigma^2 I(A_\mathbf{v} - A_\mathbf{u}) \tag{symmetry}\\[5px]
&= \sigma^2(A_\mathbf{v} - A_\mathbf{u} - A_\mathbf{v}^2 + A_\mathbf{v}A_\mathbf{u}) \\[5px]
&= \mathbf{0}\,\,. \tag{idempotence}
\end{align*}
$$
As for the distributional result,
$$
\begin{align*}
\lVert (I - A_\mathbf{v})\mathbf{Y}\rVert^2 &= \mathbf{Y}^\intercal(I - A_\mathbf{v})\mathbf{Y} \\[5px]
&= \mathbf{Y}^\intercal Q \text{ diag}(\underbrace{1, \ldots, 1}_{n-k \text{ ones}}, 0, \ldots, 0 )\, Q^\intercal \mathbf{Y} \tag{evd} \\[5px]
&= \sum_{t=1}^{n-k} (Q^\intercal \mathbf{Y})_t^2 \;\; \sim \;\; \sigma^2 \cdot \chi^2_{n-k} \tag{item 2.}
\end{align*}
$$
since eigenvalues of idempotent matrices are $0$ or $1$, $\textit{rank}(I - A_\mathbf{v}) = n-k$, and $Q^\intercal \mathbf{Y}$ is an orthogonal transformation of an iid normal vector so it is also an iid normal vector. A similar calculation reveals that 
$$
\lVert (A_\mathbf{v} - A_\mathbf{u})\mathbf{Y}\rVert^2 \;\sim \;\; \sigma^2 \cdot \chi^2_{k-1}
$$ 
since $\textit{rank}(A_\mathbf{v} - A_\mathbf{u}) = k-1$. Now we see the reason for using the F-statistic, which is a ratio of these two sums of squares: without taking the ratio, the unknown parameter $\sigma$ enters as a multiplier on the null chi-squared distributions; but after taking the (appropriately scaled) ratio, the unknown $\sigma$ cancels out and we obtain a _true statistic_ in the sense that its distribution is completely pinned down by the null hypothesis. 


