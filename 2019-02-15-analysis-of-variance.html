---
title: Analysis of variance
author: ''
date: '2019-02-15'
slug: analysis-of-variance
categories: ['Basics']
tags: []
subtitle: ''
---



<div id="f-test" class="section level2">
<h2>F-test</h2>
<p>The F distribution will play an important role in the general framework of “analysis of variance” (ANOVA). Let’s start with a simple scenario in which the F-test might arise. Suppose we observe (real-valued) data samples from several different groups</p>
<p><span class="math display">\[
\begin{align*}
\text{group }1 &amp;: Y_1^{(1)}, Y_2^{(1)}, \ldots, Y_{n_1}^{(1)} \\
\text{group }2 &amp;: Y_1^{(2)}, Y_2^{(2)}, \ldots, Y_{n_2}^{(2)} \\
&amp;\vdots \\
\text{group }k &amp;: Y_1^{(K)}, Y_2^{(K)}, \ldots, Y_{n_k}^{(k)} \\
\end{align*}
\]</span></p>
<p>and furthermore, we assume that the samples are independent draws from <span class="math inline">\(k\)</span> different Gaussians all with the same variance but potentially different means. The standard notation for this model is</p>
<p><span class="math display">\[ Y^{(i)}_j = \mu_i + \epsilon^{(i)}_j\qquad i=1,\ldots,k \qquad j=1,\ldots, n_i \qquad \epsilon^{(i)}_j \sim \mathcal{N}(0, \sigma^2)\,\,.\]</span></p>
<p>Here is an example of some data generated from this model with <span class="math inline">\(k=4\)</span> different group means and <span class="math inline">\(\sigma = 1\)</span> (with some added jittering):</p>
<p><img src="/post/2019-02-15-analysis-of-variance_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>A reasonable question would be to ask if these data provide enough evidence to conclude that the group means are different. In this setting, we can employ the following <strong>one-way ANOVA</strong> F-test statistic:</p>
<p><span class="math display">\[
F = \frac{\text{between-group variability}}{\text{within-group variability}} = \frac{\frac{1}{k-1}\left[\sum_{i=1}^k n_i (\bar{Y}^{(i)} - \widetilde{Y} )^2\right]} {\frac{1}{n-k}\left[\sum_{i=1}^k\sum_{j=1}^{n_i}(Y^{(i)}_j - \bar{Y}^{(i)})^2\right]} \tag{*}
\]</span></p>
<p>where <span class="math inline">\(n = n_1 + \ldots + n_k\)</span>, <span class="math inline">\(\widetilde{Y}\)</span> is the average of all samples, and <span class="math inline">\(\bar{Y}^{(i)}\)</span> is the average of samples within the <span class="math inline">\(i^{th}\)</span> group.</p>
<p>The numerator is a weighted sum of squared differences between the group means and the overall mean; the denominator is a sum of squared differences between each sample and its respective group mean. Intuitively, if the between-group variability is <em>substantially higher</em> than the within-group variability, the <span class="math inline">\(K\)</span> groups or clusters are “separate enough” and we would have reason to believe that the means of the groups are not all the same.</p>
<p>Under the typical ANOVA null hypothesis, we assume that the means of the <span class="math inline">\(k\)</span> groups are equal <span class="math display">\[
\mu_1 = \mu_2 = \cdots = \mu_k\,\,.
\]</span> In this setting, the F-test statistic above follows an F-distribution on <span class="math inline">\(d_1 = k-1\)</span> and <span class="math inline">\(d_2 = n-k\)</span> degrees of freedom. We reject the null if the calculated statistic is greater than <span class="math inline">\(F_{k-1, n-k, \alpha}\)</span>, the <span class="math inline">\(1-\alpha\)</span> quantile of the <span class="math inline">\(F_{k-1, n-k}\)</span> distribution.</p>
<div id="derivation-of-null-distribution" class="section level3">
<h3>Derivation of null distribution</h3>
<p>We first recall two facts:</p>
<hr />
<ol style="list-style-type: decimal">
<li><p>the F-distribution on <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> degrees of freedom arises from the ratio of two independent chi-squared random variables normalized by their respective degrees of freedom; i.e. if <span class="math inline">\(U_1 \sim \chi^2_{d_1}\)</span> and <span class="math inline">\(U_2 \sim \chi^2_{d_2}\)</span> are independent, then <span class="math display">\[
\frac{U_1/d_1}{U_2/d_2} \sim F_{d_1, d_2}\,.
\]</span></p></li>
<li><p>the chi-squared distribution on <span class="math inline">\(d\)</span> degrees of freedom arises from the sum of <span class="math inline">\(d\)</span> independent, standard normal random variables; i.e. if <span class="math inline">\(Z_i \sim \mathcal{N}(0, 1)\)</span> for <span class="math inline">\(i = 1,\ldots,d\)</span> are independent, then <span class="math display">\[
\sum_{i=1}^d Z_i^2 \sim \chi^2_d\,.
\]</span></p></li>
</ol>
<hr />
<p>The goal is to show that <span class="math inline">\((*)\)</span> is in form described in (item 1.) above. We will need a little bit of linear algebra to achieve this. Let’s define the following (random) vectors of length <span class="math inline">\(n\)</span>: <span class="math display">\[
\begin{alignat*}{4}
\mathbf{Y} &amp;= (Y^{(1)}_1, Y^{(1)}_2, \ldots, Y^{(1)}_{n_1},\quad &amp;&amp; Y^{(2)}_1, Y^{(2)}_2, \ldots,Y^{(2)}_{n_2},\quad &amp;&amp; \ldots, \quad &amp;&amp; Y^{(k)}_1, Y^{(k)}_2, \ldots ,Y^{(k)}_{n_k}) \\[5pt]
\bar{\mathbf{Y}} &amp;= ( \underbrace{\bar{Y}^{(1)} ,\quad \ldots,\quad \bar{Y}^{(1)}}_{n_1 \text{ times}}, \quad &amp;&amp; \underbrace{\bar{Y}^{(2)},\quad \ldots,\quad \bar{Y}^{(2)}}_{n_2 \text{ times}}, \quad &amp;&amp;\ldots, \quad  &amp;&amp;\underbrace{\bar{Y}^{(k)},\quad \ldots,\quad \bar{Y}^{(k)}}_{n_k \text{ times}})    \\[5pt]
\widetilde{\mathbf{Y}} &amp;= (\widetilde{Y}, \ldots \quad\ldots \quad \ldots &amp;&amp; \ldots \quad \ldots \quad \ldots &amp;&amp;\ldots &amp;&amp;\ldots \quad \ldots \quad \ldots, \widetilde{Y})
\end{alignat*}
\]</span> where <span class="math inline">\(\mathbf{Y}\)</span> is a vector of the original data, <span class="math inline">\(\bar{\mathbf{Y}}\)</span> is a vector of within-group averages, and <span class="math inline">\(\widetilde{\mathbf{Y}}\)</span> is a vector of the overall average. It’s clear that <span class="math inline">\(\bar{\mathbf{Y}}\)</span> and <span class="math inline">\(\widetilde{\mathbf{Y}}\)</span> are linear combinations of the original, but to make this more precise, consider the following (fixed) vectors in <span class="math inline">\(\mathbb{R}^n\)</span>: <span class="math display">\[
\begin{align*}
\mathbf{v}_1 &amp;= (\underbrace{1, \ldots,  1}_{n_1 \text{ ones}}, \quad \underbrace{0, \ldots, 0}_{\text{ rest are zeros}}) \\[5px]
\mathbf{v}_2 &amp;= (\underbrace{0, \ldots, 0}_{n_1 \text{ zeros}}, \quad \underbrace{1, \ldots, 1}_{n_2 \text{ ones}}, \quad\underbrace{0, \ldots, 0}_{\text{rest are zeros}}) \\[5px]
\mathbf{v}_3 &amp;= (\underbrace{0, \ldots, 0, \quad 0, \ldots, 0}_{n_1 + n_2 \text{ zeros}}, \quad\underbrace{1, \ldots, 1}_{n_3 \text{ ones}}, \quad \underbrace{0, \ldots, 0}_{\text{rest are zeros}}) \\
&amp;\vdots \\[5px]
\mathbf{v}_k &amp;= (\underbrace{0, \ldots, 0}_{n - n_k \text{ zeros}},\quad \underbrace{1, \ldots, 1}_{n_k \text{ ones}}) \\[7px]
\mathbf{u} &amp;= \mathbf{v}_1 + \cdots + \mathbf{v}_k = (\underbrace{1, \ldots, 1}_{n \text{ ones}})\,\,.
\end{align*}
\]</span></p>
<p>Now we make two important observations: <span class="math inline">\(\bar{\mathbf{Y}}\)</span> is the orthogonal projection of the data <span class="math inline">\(\mathbf{Y}\)</span> onto <span class="math inline">\(\textit{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_2, \ldots, \mathbf{v}_k\}\)</span>, and <span class="math inline">\(\widetilde{\mathbf{Y}}\)</span> is the orthogonal projection of the data <span class="math inline">\(\mathbf{Y}\)</span> onto <span class="math inline">\(\textit{span}\{\mathbf{u}\}\)</span>. We can denote this succinctly as <span class="math display">\[
\bar{\mathbf{Y}} = A_{\mathbf{v}} \mathbf{Y} \quad,\quad \widetilde{\mathbf{Y}} = A_{\mathbf{u}} \mathbf{Y}
\]</span> where <span class="math inline">\(A_{\mathbf{v}}\)</span> and <span class="math inline">\(A_{\mathbf{u}}\)</span> are the appropriate orthogonal projection matrices (so they will be symmetric and idempotent). The pre-scaled numerator of the F-test statistic <span class="math inline">\((*)\)</span> can now be written<br />
<span class="math display">\[
\sum_{i=1}^k n_i (\bar{Y}^{(i)} - \widetilde{Y} )^2 = \lVert \bar{\mathbf{Y}} - \widetilde{\mathbf{Y}}\rVert^2 = \lVert A_\mathbf{v}\mathbf{Y} - A_\mathbf{u}\mathbf{Y}\rVert^2
\]</span> and the pre-scaled denominator can be written <span class="math display">\[
\sum_{i=1}^k\sum_{j=1}^{n_i}(Y^{(i)}_j - \bar{Y}^{(i)})^2 = \lVert \mathbf{Y} - \bar{\mathbf{Y}} \rVert^2 = \lVert \mathbf{Y} - A_\mathbf{v}\mathbf{Y}\rVert^2\,\,.
\]</span> As a geometric side note, since <span class="math inline">\(\textit{span}\{\mathbf{u}\} \subset \textit{span}\{\mathbf{v}_1, \ldots, \mathbf{v}_k\}\)</span>, the orthogonal projections above yield a pythagorean identity <span class="math display">\[
\lVert \mathbf{Y} - \widetilde{\mathbf{Y}}\rVert^2 = \lVert \mathbf{Y} - \bar{\mathbf{Y}}\rVert^2 + \lVert \bar{\mathbf{Y}} - \widetilde{\mathbf{Y}}\rVert^2\,\,.
\]</span> This can conveniently be interpreted as a partition of variability: we are splitting the variation in all the data into variation <em>within</em> groups and variation <em>between</em> groups. We are now tasked with showing that the random variables on right-hand side are (stochastically) independent and follow chi-squared distributions. The independence follows easily from assumptions of the data generation model and the null hypothesis (which purports that the group means are equal, i.e. the data are iid): <span class="math display">\[
\begin{align*}
\text{cov}(\mathbf{Y} - \bar{\mathbf{Y}}, \bar{\mathbf{Y}} - \widetilde{\mathbf{Y}}) &amp;= 
\text{cov}((I - A_\mathbf{v})\mathbf{Y}, (A_\mathbf{v} - A_\mathbf{u})\mathbf{Y}) \\[5px]
&amp;= (I - A_\mathbf{v})\text{cov}(\mathbf{Y}, \mathbf{Y})(A_\mathbf{v} - A_\mathbf{u})^\intercal \\[5px]
&amp;= (I - A_\mathbf{v})\sigma^2 I(A_\mathbf{v} - A_\mathbf{u}) \tag{symmetry}\\[5px]
&amp;= \sigma^2(A_\mathbf{v} - A_\mathbf{u} - A_\mathbf{v}^2 + A_\mathbf{v}A_\mathbf{u}) \\[5px]
&amp;= \mathbf{0}\,\,. \tag{idempotence}
\end{align*}
\]</span> As for the distributional result, <span class="math display">\[
\begin{align*}
\lVert (I - A_\mathbf{v})\mathbf{Y}\rVert^2 &amp;= \mathbf{Y}^\intercal(I - A_\mathbf{v})\mathbf{Y} \\[5px]
&amp;= \mathbf{Y}^\intercal Q \text{ diag}(\underbrace{1, \ldots, 1}_{n-k \text{ ones}}, 0, \ldots, 0 )\, Q^\intercal \mathbf{Y} \tag{evd} \\[5px]
&amp;= \sum_{t=1}^{n-k} (Q^\intercal \mathbf{Y})_t^2 \;\; \sim \;\; \sigma^2 \cdot \chi^2_{n-k} \tag{item 2.}
\end{align*}
\]</span> since eigenvalues of idempotent matrices are <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, <span class="math inline">\(\textit{rank}(I - A_\mathbf{v}) = n-k\)</span>, and <span class="math inline">\(Q^\intercal \mathbf{Y}\)</span> is an orthogonal transformation of an iid normal vector so it is also an iid normal vector. A similar calculation reveals that <span class="math display">\[
\lVert (A_\mathbf{v} - A_\mathbf{u})\mathbf{Y}\rVert^2 \;\sim \;\; \sigma^2 \cdot \chi^2_{k-1}
\]</span> since <span class="math inline">\(\textit{rank}(A_\mathbf{v} - A_\mathbf{u}) = k-1\)</span>. Now we see the reason for using the F-statistic, which is a ratio of these two sums of squares: without taking the ratio, the unknown parameter <span class="math inline">\(\sigma\)</span> enters as a multiplier on the null chi-squared distributions; but after taking the (appropriately scaled) ratio, the unknown <span class="math inline">\(\sigma\)</span> cancels out and we obtain a <em>true statistic</em> in the sense that its distribution is completely pinned down by the null hypothesis.</p>
</div>
</div>
