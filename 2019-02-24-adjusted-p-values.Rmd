---
title: P-values and control of FWER
author: ''
date: '2019-02-24'
categories:
  - Basics
slug: P-values and control of FWER
---

### Derivation

P-values are ubiquitous in the world of inference. Formally, a _p-value_ is constructed as follows. Suppose we observe (for simplicity, real-valued) data
$$
Y = (Y_1, Y_2, \ldots, Y_n) \; \; \sim \; \; P_\theta
$$
where, as experienced modelers, we have settled on some family of candidate probability measures on $\mathbb{R}^n$ parameterized by the vector $\theta$. We may wish to test the credibility of a null hypothesis
$$
\mathcal{H}_0 : \;\; \theta \in \Theta_0
$$
and suppose that we have cleverly identified a real-valued statistic $g(Y)$ whose distribution _under this null hypothesis_ is fully-specified; i.e. we can unambiguously compute
$$
P_0(B) \doteq \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}}\Big[g(Y) \in B \Big]
$$
and we write $g(Y) \sim P_0$, where $P_0$ doesn't depend on the unknown $\theta$ anymore. For the remainder of this post, we assume that $g(Y)$ is a continuous random variable; i.e. $P_0$ is absolutely continuous with respect to the Lebesgue measure on $\mathbb{R}$. 

When should we reject the null hypothesis? It would be sensible to do so if we observe a "rare" value of $g(Y)$ under the null distribution $P_0$. Usually we take the significance level $\alpha = .05$ as our "rare" threshold. So we might define a rejection region of the form $(t, \infty)$ such that 
$$
P_0\Big((t, \infty)\Big) = \text{Pr}_{_{Z\sim P_0}} \Big[Z > t \Big] = \alpha\,\,.
$$
From this, we see that the statistic that we computed on the data satisfies
$$
g(Y) > t  \;\; \iff \;\; \text{Pr}_{_{Z\sim P_0}}\Big[Z > g(Y) \Big] \le \alpha\,\,.
$$
This, then, yields the <u>definition of p-value</u>:
$$
p(Y) \doteq \text{Pr}_{_{Z\sim P_0}}\Big[Z > g(Y) \Big] = 1 - F_0(g(Y))\,\,.
$$
where $F_0$ is the (continuous) distribution function for the measure $P_0$. It is fruitful to compare this with the colloquial interpretation: __"the p-value is the chance of observing, on many similar datasets, an effect at least as extreme as the one in our sample data, assuming the null hypothesis is true"__.  

We can see that under the null hypothesis, the p-value follows a uniform distribution on $[0,1]$ since for any $t\in [0,1]$ we have
$$
\begin{align*}
\text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[ p(Y) \le t\Big] &= 
\text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[ 1 - F_0(g(Y)) \le t\Big] \\[5px]
&= \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[F_0(g(Y)) \ge 1-t\Big] \\[5px]
&= 1 - \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[F_0(g(Y)) < 1-t\Big] \\[5px]
&= 1 - \text{Pr}_{_{Y \sim P_\theta,\, \mathcal{H}_0}} \Big[g(Y) \le F_0^{-1}(1-t)\Big] \\[5px]
&= 1 - F_0(F_0^{-1}(1-t)) \\[5px]
&= t
\end{align*}
$$
where the last step follows by continuity of $F_0$. And clearly this is precisely the distribution function of the $\text{unif}(0,1)$ measure. 

So if we reject the null hypothesis when $p(Y) \le \alpha$, we do so with probability $\alpha$ under the null model. Therefore, in this setting the Type 1 error, or __"the probability of rejecting the null when the null is true"__, is $\alpha$.  

### Bonferroni correction

Now let's take a look at what happens when we conduct multiple tests simultaneously. In other words, from our sample data, we would like to determine whether there is evidence enough to reject a collection of null hypotheses. For each null hypothesis $\mathcal{H}_i$ we formulate a statistic $g_i(Y)$ and compute the corresponding p-value $p_i(Y)$:
$$
\begin{alignat*}{4}
&\mathcal{H}_1\qquad &&\mathcal{H}_2 \qquad &&\ldots \qquad &&\mathcal{H}_m \\[5px]
&\downarrow && \downarrow  && && \downarrow \\[5px]
& p_1(Y) && p_2(Y) && && p_m(Y)
\end{alignat*}
$$

For simplicity, suppose that we insist on performing a test of the same form as the single hypothesis setting discussed above: we reject those null hypotheses for which $p_i(Y) \le \gamma$ for some predetermined $\gamma$. What should we pick as our $\gamma$? This depends on the kind of control we wish to achieve. Previously, with a single hypothesis, we were controlling the _probability of rejecting a true null_. A natural generalization of this principle would be to control the probability of rejecting any true nulls; in other words, we could control the _probability of rejecting at least one true null_. This is known as the __familywise error rate (FWER)__.

Let's formalize this concept. Null hypotheses are <u>statements about the location of the true underlying parameter $\theta$ </u>. For example, if the parameter space is $\mathbb{R}^2$, null hypotheses correspond to subsets of the plane with each hypothesis asserting that $\theta$ lies in its subset. In the picture below, the black dot is $\theta$ so we see that $2$ of the $3$ null hypotheses are true. 

<iframe src="https://www.desmos.com/calculator/4laiy3c8is?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>


If $m_0$ out of the $m$ null hypotheses are true, we can write the FWER as follows:

$$
\begin{align*}
\text{FWER} &= \text{Pr}_{_{Y\sim P_\theta}} \Big[ \text{reject at least one true null} \Big] \\[5px] 
&= \text{Pr}_{_{Y\sim P_\theta}} \Bigg[\bigcup_{i: \,\mathcal{H_i} \text{ true}} \Big\{p_i(Y) \le \gamma \Big\} \Bigg] \\[5px]
&\le \sum_{i: \,\mathcal{H_i} \text{ true}} \text{Pr}_{_{Y\sim P_\theta, \,\mathcal{H}_i}} \Big[ p_i(Y) \le \gamma  \Big] \\[5px]
&= m_0\gamma\,\,.
\end{align*}
$$
So to control the FWER at level $\alpha$, it suffices to select $\gamma$ such that $m_0 \gamma \le \alpha$. However, we don't know $m_0$! Despite this, since $m_0 \le m$, we can make the conservative choice $\gamma = \frac{\alpha}{m}$ which satisfies the desired inequality:
$$
\text{FWER} \le m_0 \gamma = m_0 \bigg(\frac{\alpha}{m}\bigg) \le m \bigg( \frac{\alpha}{m} \bigg) = \alpha \,\,.
$$
This procedure usually controls the FWER at a level substantially lower than $\alpha$, so we won't be rejecting many nulls. 


### Holm method

This procedure is uniformly more powerful than the Bonferroni correction, in the sense that it still controls the FWER at the $\alpha$ level, but if Bonferroni rejects a particular null then so does Holm. We will elaborate on this point further, but first we present the procedure itself. For ease of notation, we will suppress the p-value's dependence on the data and write $p_i$ instead of $p_i(Y)$. 

***

1. Sort the $m$ p-values in ascending order: $p_{(1)} \le p_{(2)} \le \cdots \le p_{(m)}$, with corresponding null hypotheses $\mathcal{H}_{(1)}, \mathcal{H}_{(2)}, \cdots, \mathcal{H}_{(m)}$. 

2. For a fixed significance level $\alpha$, let $k$ be the minimal index such that 
$$p_{(k)} > \frac{\alpha}{m - (k-1)}\,\,.$$
Let's set $k = 0$ if no such index exists. Note that by definition $k$ is _random_ and depends on the data $Y$. 
3. Reject $\mathcal{H}_{(1)}, \ldots, \mathcal{H}_{(k-1)}$ and do not reject $\mathcal{H}_{(k)}, \ldots , \mathcal{H}_{(m)}$. Note that the corresponding p-values satisfy 
$$
p_{(1)} \le \frac{\alpha}{m}, \quad p_{(2)} \le \frac{\alpha}{m-1}, \quad \ldots \quad,  \quad p_{(k-1)} \le  \frac{\alpha}{m -(k-2)}, \quad p_{(k)} >  \frac{\alpha}{m - (k-1)}\,.
$$
If $k=1$ then we don't reject any hypotheses, and if $k=0$, then we reject all of the hypotheses. 

***

The steps are simple enough, but does this achieve our goal of controlling the FWER? To see this, let $t$ be the index of the <u>first rejected true hypothesis</u>, with respect to the sorted order of p-values above. To be thorough, we set $t = 0$ if no true hypotheses are rejected. This is another _random index_, and if $t \ne 0$, it will satisfy 
$$
p_{(t)} \le \frac{\alpha}{m - (t - 1)} \le \frac{\alpha}{m - (m - m_0)} = \frac{\alpha}{m_0} 
$$
since the first $t-1$ rejected hypotheses are false null hypotheses and there are $m - m_0$ of these in total.

Therefore, we have the following implication: if any true null hypotheses are rejected, the smallest of the corresponding p-values must be $\le \frac{\alpha}{m_0}$. Even more generally, at least one of the $m$ p-values must be $\le \frac{\alpha}{m_0}$. This yields the desired result: 
$$
\begin{align*}
\text{FWER} &= \text{Pr}_{_{Y\sim P_\theta}} \Big[ \text{reject at least one true null} \Big] \\[5px]
&= \text{Pr}_{_{Y\sim P_\theta}} \Big[ t \ne 0 \Big] \\[5px]
&\le \text{Pr}_{_{Y\sim P_\theta}} \Bigg[\bigcup_{i: \,\mathcal{H_i} \text{ true}} \bigg\{p_i \le \frac{\alpha}{m_0} \bigg\} \Bigg] \\[5px]
&\le \sum_{i: \,\mathcal{H_i} \text{ true}} \text{Pr}_{_{Y\sim P_\theta, \,\mathcal{H}_i}} \bigg[ p_i \le \frac{\alpha}{m_0}  \bigg] \\[5px]
&= m_0 \bigg(\frac{\alpha}{m_0} \bigg) \\[5px]
&= \alpha \;\;.
\end{align*}
$$

Let's return briefly to our claim that Holm is more powerful than Bonferroni; we can compare the regions corresponding to "rejection of at least one true null" between the two procedures and see that

$$
\left(\bigcup_{i: \,\mathcal{H_i} \text{ true}} \bigg\{p_i\le \frac{\alpha}{m} \bigg\} \right)\subseteq
\Big\{ t \ne 0  \Big\}\;\;.
$$
Or in words: if $p_i \le \frac{\alpha}{m}$ for some true hypothesis $\mathcal{H}_i$ causing a Bonferroni rejection, then $t\ge 1$ and in particular, $\mathcal{H}_i$ is rejected by Holm by inspection of step (3.) of the procedure.  












